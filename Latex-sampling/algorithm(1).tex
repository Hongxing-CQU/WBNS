\section{Algorithm}

In the paper,
we take a single class blue noise samples  as the wasserstein barycenter with the constrained probability measure.
Multi-class blue noise samples are thought of as the wasserstein barycenter with the joint probability measure.
\subsection{Single class blue noise sampling}
Given an arbitrary space $\Omega$ and a density function $\rho(x)$ on the space $\Omega$,
a probability measure is defined as:
\begin{equation}\label{probability-measure}
  \nu(U)=\int_U\rho(x)dx, U\subseteq\Omega
\end{equation}
Sampling the density function $\rho(x)$ consists of picking a few representative points $x_i$
that capture $\rho$ well.
These representative points $x_i$ represents a discrete probability measure,
\begin{equation*}
\begin{split}
  \mu=\sum\limits_{i=1}^n\rho_i\delta_{x_i} \quad
  s.t. \sum\limits_{i=1}^n\rho_i=1,  \delta_{x_i}= \left\{ \begin{array}{ll}
  1 & x_i\in\Omega\\
  0 & \textrm{others}
  \end{array} \right.
  \end{split}
\end{equation*}
where $n$ is the number of samples,
$\rho_i$ is the probability at $x_i$.
Thus,
sampling means that $\mu$ captures $\nu$ well.
We can use Wasserstein barycenter to model a single class sampling as:
\begin{equation}\label{single-class-problem}
\begin{split}
 \mu=arg\min\limits_\mu W_p^p(\mu,\nu)=\inf\limits_\mu\int_{X\times\Omega}d(x_i,y)^pd\pi(x_i,y)\\
 s.t. \int_\Omega\pi(x_i,x)=\rho_i, \sum\limits_{i=1}^n\int_{U\subseteq\Omega}\pi(x_i,y)dy=\nu(U)
 \end{split}
\end{equation}
where $X=\{x_1,x_2,...,x_n\}\subseteq\Omega$.

Compared with previous methods on CCVT,
Wasserstein barycenter provides a more general framework for sampling.
Previous methods on CCTV are special cases of Eq.~\ref{single-class-problem} with an implicit constrain on  transport plan $\pi$,
that is
\begin{equation}\label{eq:transform-plan}
  \pi(x_i,y)\cdot\pi(x_j,y)=0 \quad(i\neq j).
\end{equation}
Eq.~\ref{transform-plan} means that the mass in a certain domain can only be transported to one sampling point.
For example,
the corresponding domain of sampling points is a few discrete points in~\cite{balzer:2009:capacity},
and the corresponding domain of sampling points is the power diagram region of sampling points in~\cite{de:2012:blue}.
In fact,
Eq.~\ref{transform-plan} is a very strict condition for the variational problem Eq.~\ref{single-class-problem}.
It can only be satisfied when $\nu$ is a continuous probability measure.
It also introduce "failure case" for multi-class sampling~\cite{wei:2010:multi}.
We will discuss the reason in the next subsection.

\subsection{Multi-class blue noise sampling}
For multi-class sampling,
we will compute $m$-class sampling points for $m$-class probability density functions.
Let $\{\rho_1,\cdots,\rho_m\}$ be a series of individual probability density functions,
which will be sampled, on the space $\Omega$.
a combined probability density function $\rho_{i_1,i_2,...,i_k}$ can be generated by $k(k\leq m)$ different probability density functions $\rho_{i_1},\rho_{i_2},...,\rho_{i_k}$,
that is
\begin{equation*}
\rho_{i_1,i_2,...,i_k}=(\rho_{i_1}+\rho_{i_2}+...,+\rho_{i_k})/k
\end{equation*}
Assumed sampled points of probability function $\rho_i$ is $X_i$,
\begin{equation*}
  X_i=\{x_i^1,x_i^2,...,x_i^{n_i}\}\quad(1\leq i\leq m), x_i^j\in\Omega(1\leq j\leq n_i).
\end{equation*}
For multi-class blue noise sampling,
each individual class as well as combined classes exhibit blue noise characteristics~\cite{wei:2010:multi}.
It means that every class sampling points $X_i$ captures the probability function $\rho_i$ well,
and combined sampling points $\{X_{i_1}, X_{i_2},...,X_{i_k}\}$ captures the probability density function $\rho_{i_1,i_2,...,i_k}$ well too.

In terms of Eq.~\ref{probability-measure},
there is a corresponding probability measure $\nu_i(1\leq i\leq 2^m-1)$for every $\rho_i$ or every  $\rho_{{i_1,i_2,...,i_k}}$.
For every $X_i$ or every $\{X_{i_1}, X_{i_2},...,X_{i_k}\}$, there is a corresponding discrete probability measure $\mu_i(1\leq i\leq 2^m-1)$.
Every discrete probability measure $\mu_i$ should approximate corresponding probability measure $\nu_i$.

Compared with single class sampling,
we cannot compute an optimal $\mu_i$ for every $\nu_i$ in terms of Eq.~\ref{single-class-problem} independently,
since sampling points are influenced by individual probability measures and combined probability measures simultaneously.
We describe multi-class sampling as a combined Wasserstein barycenter,
\begin{equation}\label{eq:combined-wasserstein-barycenter}
 \bar\mu=arg\min\limits_{\bar\mu}\sum\limits_{i=1}^K\lambda_iW_p^p(\mu_i,\nu_i) \quad
  s.t.\sum\limits_{i=1}^K\lambda_i=1,(\lambda_i\geq0)
\end{equation}
where $\bar\mu=\{\mu_1,...,\mu_m,\mu_{m+1},...\mu_{K}\}$,
$\mu_i(1\leq i\leq m)$  is a discrete probability measure corresponding to $X_i$,
$\mu_i(m < i\leq K)$ is a discrete probability measure corresponding to combined sampling points.

In extended version of~\cite{wei:2010:multi},
Wei pointed out that multi-class relaxation might get stuck in local minimums with insufficient sample
uniformity for a simple reason: for a given sample $s$,
different class combinations may have different opinions about the desired centroid
location to which $s$ should move.
In essence,
conflicts of desired centroid locations are introduced by the constrain on transport plans $\pi$ (Eq.~\ref{eq:transform-plan}).
In combined Wasserstein barycenter~\ref{eq:combined-wasserstein-barycenter},
the constrain is broken via a general transport plan $\pi$.
First,
$\pi$ is a global transport plan other than a local transport plan,
which is constrained in a local partition,
such as discrete points~\cite{balzer:2009:capacity},
Voronoi diagram~\cite{chen:2012:variational} or Power diagram~\cite{de:2012:blue}.
Second,
$\pi$ is a separable transport plan,
which means the mass located in a small region can be transported to different samples other than only one sample.
Furthermore,
we relax transport plans via entropic regular terms as~\cite{cuturi:2013:sinkhorn}.
Regulated Wasserstein distance is defined as
\begin{equation}\label{eq:regulate-wasserstein-distance}
 W_p(\mu,\nu)=\left(\inf\limits_{\pi\in\prod(\mu,\nu)}\int_{\Omega^2}d(x,y)^pd\pi(x,y)+\epsilon H(\pi)\right )^{1/p}
\end{equation}
where $\epsilon$ is a positive regularization parameter,
and $H(\pi)$ is the entropy of $\pi$.
More details can be found in~\cite{cuturi:2013:sinkhorn}.

\subsection{Numerical Optimization}
Eq.~\ref{eq:combined-wasserstein-barycenter} is a unified variational model for single class sampling and multi-class sampling.
For every discrete probability measure $\mu_i\in\bar\mu$
we represent it as:
\begin{equation}
 \mu_i=\sum\limits_{j=1}^{n_i}\rho_i^j\delta_{x_i^j} \quad s.t. \sum\limits_{j=1}^{n_i}\rho_i^j=1.
\end{equation}
where $\rho_i^j$ is probability at sampling point $x_i^j$ for the $i$th-class sampling
,$n_i$ is the number of samples for the $i$th-class sampling.
For blue noise sampling,
$\rho_i^j$ is a constant and $\rho_i^j=1/n_i$.
Thus, we need only to compute variables $X_i=\{x_i^1,...,x_i^{n_i}\}$ in Eq. ~\ref{eq:combined-wasserstein-barycenter} .

Every probability measure $\nu_k$ in Eq.~\ref{eq:combined-wasserstein-barycenter} may be a continuous probability measure or a discrete probability measure.
If $\nu_k$ is a continuous probability measure,
we must discretize  $\nu_k$ as a discrete probability measure
to compute integrations required in Eq. ~\ref{eq:combined-wasserstein-barycenter} by quadrature.
\begin{equation}
\nu_k=\sum\limits_{j=1}^{m_k}\varrho_k^j\delta_{y_k^j} \quad s.t. \sum\limits_{j=1}^{m_k}\varrho_k^j=1
\end{equation}
where $\varrho_k^j=\varrho_k(y_k^j)V_{y_k^j}$
and $V_{y_k^j}$ is the area with density $\varrho_k(y_k^j)$ when the density function
$\varrho_k$ is approximated with a positive piecewise constant function,
$\varrho_k$ is the probability density function of the probability measure $nu_k$,
and $m_k$ is the number of discrete points of the $k$th-class probability measure $\nu_k$.

Based on discrete representation of probability measures and regularized Wasserstein distance (Eq.~\ref{eq:regulate-wasserstein-distance}),
variational problem (Eq.~\ref{eq:combined-wasserstein-barycenter}) can be represented as:
\begin{equation}\label{eq:discrete-multi-problem}
  \begin{aligned}
  \mathbf{X}=\arg\min\limits_{\mathbf{X}}\sum\limits_{k=1}^K\lambda_k<\mathbf{D}_k,\mathbf{\Pi}_k> \\
    s.t.\ \ \mathbf{\Pi}_k\mathbf{1}=\mathbf{\varrho}_k,\ \mathbf{\Pi}_k^T\mathbf{1}=\mathbf{\rho}_k
  \end{aligned}
\end{equation}
where $\mathbf{X}=\{X_1,...,X_m\}$ is the set of all samples,
 $\mathbf{D}_k,\mathbf{\Pi}_k\in R_+^{n_k\times m_k}$,
 $\mathbf{D}_k$ is the distance matrix and $D_k^{(i,j)}=d(x_k^i,y_k^j)^p$,
 $\mathbf{\Pi}_k$ is the transport cost matrix and $\Pi(x_k^i,y_k^j)$ represents the mount of mass transported from $x_k^i$ to $y_k^j$,
 and $<\cdot,\cdot>$ is Frobenius product of two matrixes.

 We proceed with samples generation by computing a optimal solution of the variational problem (Eq.\ref{eq:discrete-multi-problem}).
 We apply a loop iteration algorithm to extremize the variational problem by repeatedly performing a minimization step over positions $\mathbf{X}$ followed by a projection step over transport plans $\mathbf{\Pi}_k$.

 For a fixed set of points $\mathbf X$,
 we compute  relaxed transport plans via regulated Wasserstein distance (Eq.\ref{eq:regulate-wasserstein-distance}).
 For every transport plan matrix $\mathbf{\Pi}_k$,
 Eq.\ref{eq:regulate-wasserstein-distance} can be discretized as
 \begin{equation}\label{eq:discrete-regulate-wasserstein-barycenter}
   W_p^p(\mathbf{D}_k,\mathbf{\Pi}_k)=arg\min\limits_{\mathbf{\Pi}_k}<\mathbf{D}_k,\mathbf{\Pi}_k>-\epsilon H(\mathbf{\Pi}_k)
 \end{equation}
The optimal transport plan $\mathbf{\Pi}_k$ can be obtained by iterative Bregman projection~\cite{cuturi:2013:sinkhorn,benamou:2015:iterative}, and
\begin{equation}\label{eq:transport-plan}
  \mathbf{\Pi}_k=diag(\mathbf{u})\mathbf{\xi}diag(\mathbf{v})
\end{equation}
where $\mathbf{\xi}=e^{-\frac{\mathbf{D}_k}{\epsilon}}$,
$\mathbf{u}\in \mathbb{R}_+^{n_k}$ and $\mathbf{v}\in\mathbb{R}_+^{m_k}$.
More details can be found in~\cite{cuturi:2013:sinkhorn,cuturi:2013:fast,benamou:2015:iterative}.

Assumed that $\Pi_k(k=1,...,K)$ is fixed,
variational formula Eq.~\ref{eq:discrete-regulate-wasserstein-barycenter}
is a quadratic function about $\mathbf(X)$ with a piecewise linear concave function when $p=2$.
$\mathbf{X}$ can be obtained by Newton iterative method,
\begin{equation}\label{eq:position-iterative}
  \mathbf{X}\leftarrow(1-\theta)\mathbf{X}+\theta\mathbf{X}\mathbf{\pi}_kdiag({\varrho_k^{i,j}}^{-1}) \quad (0\leq\theta\leq 1)
\end{equation}
To improve randomness of samples,
we generate a random number for $\theta$ in every iteration.


\begin{figure}[htb]
\rule{0.5\textwidth}{0.4pt}
 \quad 01: //Multi-class blue noise as Wasserstein barycenter\\
\quad 02: Input: domain $\Omega$, densities $\nu_1,...,\nu_n$, and
  number of points $N_1,...,N_n$\\
\quad 03: Output: coordinates $\bar X=\{x_{1,1} \cdots  x_{1,N_1} \cdots x_{n,1} \cdots x_{n,N_n}\}$  of sampled points\\
\quad 04: Initialize $\bar X$ with random points inside $\Omega$ conforming to $\rho_1,...,\rho_n$ \\
\quad 05: Initialize parameters $\epsilon$ for the entropic regularization\\
%\quad 6: Initialize the parameter $t$ for the simulated annealing strategy\\
\quad 06: Initialize transport cost matrixes $\Pi_k=\frac{1}{n_mN_k}\mathbf{I}$, $\mathbf{I}$ is the identity matrix\\
\quad 07: Compute the energy $E_0=\sum_{k=1}^K\lambda_k<D,\Pi_k>$\\
\quad 08: \textbf{Repeat}\\
09:\quad Compute the distance matrix $\mathbf{D}$ \\
10:\quad // Iterative bregman projections for $\Pi_k^*$\\
11:\quad For k=1:K \\
12:\quad \quad  Iterative-bregman-projection() (lines 22-30) \\
13:\quad // Newton iterative method for $\bar X$ \\
14:\quad Generate K random numbers $r_1,...,r_K\in[0,1]$\\
15:\quad Normalize $r_1,...,r_K$,$\sum_{k=1}^Kr_=1$\\
16:\quad Update $\bar X$, $\bar X=(1-\theta)\bar X+\theta\sum_{k=1}^K\lambda_kr_kX\Pi_k^Tdiag(\frac{1}{N_k})$\\
17:\quad Update $D$\\
18:\quad $E_1=\sum_{k=1}^K\lambda_k<D,\Pi_k>$ \\
19:\quad $dE=E_1-E_0$\\
20:\quad Update $\epsilon$ \\
21: \textbf{Until} $dE$ is convergent \\

22: Subroutine Iterative bregman projections()\\
23: Input: the distance matrix $D$, the parameter $\epsilon$,
    the density function $\mathbf{p}=[\rho_1^k \rho_2^k \cdots \rho_M^k]$,
    $\mathbf{q}=[\rho_{1,1}^k \cdots \rho_{1,N_1}^k \cdots \rho_{n,1}^k \cdots \rho_{n,N_n}^k]$\\
24: Initiate $i=0$, $v^i=\mathbf{1}$\\
25: Initiate $\mathbf\xi=e^{-\mathbf{D}/\epsilon}$\\
26: \textbf{Repeat}\\
27:\quad $u^i=\frac{\mathbf{p}}{\xi v^i}$\\
28:\quad $\Pi_k^i=diag(u^i)\xi diag{v^i}$\\
29:\quad $i=i+1$,$v^i=\frac{\mathbf{q}}{\xi^Tu^i}$\\
30: \textbf{Until} $\Pi_k^i$ is convergent\\
\rule{0.47\textwidth}{0.5pt}
  \caption{Pseudocode of multi-class blue noise algorithm}\label{}
\end{figure}


%To avoid the algorithm to stuck in local minima,
%a simulated annealing strategy is introduced to the gradient descent method.

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}\textwidth
  \subfigure[{Dart throwing [Wei 2010]}]{

  \includegraphics[width=0.85\textwidth]{figure/3-class-weiliyi.png}}
  \subfigure[Our method]{

  \includegraphics[width=0.85\textwidth]{figure/3-class-our.png}}
  \caption{The comparison of Wei's algorithm and our algorithm for three-class blue noise sampling.
  $\lambda_{1,2,3,4,5,6,7}=1/18,1/18,1/18,2/18,2/18,2/14,9/18$.
  $\lambda_1$,$\lambda_2$ and $\lambda_3$ are weighted parameters for each individual class.
  $\lambda_1$,$\lambda_2$ and $\lambda_3$ are weighted parameters for combined classes with two individual class.
  $\lambda_8$ is the weighted parameters for the total samples.
  The number of samples of each individual class is $1024$.}
  \label{three-class-sampling}
\end{figure*}