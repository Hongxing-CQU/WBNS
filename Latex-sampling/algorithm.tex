\section{Algorithm}

In the paper,
we take a single class blue noise samples  as the wasserstein barycenter with the constrained probability measure.
Multi-class blue noise samples are chosen as the wasserstein barycenter with the combined probability measure.
\subsection{Single class blue noise sampling}
Given an arbitrary space $\Omega$ and a density function $\varrho(x)$ on the space $\Omega$,
a probability measure is defined as:
\begin{equation}\label{probability-measure}
  \begin{split}
  \nu(U)=\int_U\varrho(x)dx, U\subseteq\Omega \\
  s.t. \int_{\Omega}\varrho(x)dx=1,\varrho(x)\geq0
  \end{split}
\end{equation}
Sampling the density function $\varrho(x)$ consists of picking a few representative points $x_i$
that capture $\varrho$ well.
These representative points $x_i$ represents a discrete probability measure,
\begin{equation*}
\begin{split}
  \mu=\sum\limits_{i=1}^n\rho_i\delta_{x_i} \quad
  s.t. \sum\limits_{i=1}^n\rho_i=1,  \delta_{x_i}= \left\{ \begin{array}{ll}
  1 & x_i\in\Omega\\
  0 & \textrm{others}
  \end{array} \right.
  \end{split}
\end{equation*}
where $n$ is the number of samples,
$\rho_i$ is the probability at $x_i$.
Thus,
sampling means that $\mu$ captures $\nu$ well.
We can use Wasserstein barycenter to model a single class sampling as:
\begin{equation}\label{single-class-problem}
\begin{split}
 \mu=arg\min\limits_\mu W_p^p(\mu,\nu)=\inf\limits_\mu\int_{X\times\Omega}d(x_i,y)^pd\pi(x_i,y)\\
 s.t. \int_\Omega\pi(x_i,x)=\rho_i, \sum\limits_{i=1}^n\int_{U\subseteq\Omega}\pi(x_i,y)dy=\nu(U)
 \end{split}
\end{equation}
where $X=\{x_1,x_2,...,x_n\}\subseteq\Omega$.

Compared with previous methods on CCVT~\cite{balzer:2009:capacity,de:2012:blue},
Wasserstein barycenter provides a more general framework for sampling.
Previous methods on CCVT are special cases of Eq.~\ref{single-class-problem} with an implicit constraint on  transport plan $\pi$,
that is
\begin{equation}\label{eq:transform-plan}
  \pi(x_i,y)\cdot\pi(x_j,y)=0 \quad(i\neq j).
\end{equation}
Eq.~\ref{eq:transform-plan} means that the mass in a certain domain can only be transported to one sampling point.
For example,
the corresponding domain of sampling points consists of a few discrete points in~\cite{balzer:2009:capacity},
and the power diagram region of sampling points in~\cite{de:2012:blue}.
In fact,
Eq.~\ref{eq:transform-plan} is a very strict condition for the variational problem in Eq.~\ref{single-class-problem}.
It can only be satisfied when $\nu$ is a continuous probability measure.
It also introduces "failure case" for multi-class sampling~\cite{wei:2010:multi}.
We will discuss the reason in the next subsection.

\subsection{Multi-class blue noise sampling}
%For multi-class sampling,
%we will compute $N$-class sampling points for $m$-class probability density functions,
%where $N$ is the number of individual probability density functions and
%$m-N(m>N)$ is the number of combined probability density functions.
For multi-class blue noise sampling,
both each individual class and combined classes exhibit blue noise characteristics~\cite{wei:2010:multi}.
It is necessary to consider individual probability distributions and combined probability distributions simultaneously
in multi-class sampling. \\
%sampling points are drawn from $N$-class individual probability distribution
%and $M$-class combined probability distribution.
Let $\{\varrho_1,\cdots,\varrho_N\}$ be $N$ individual probability density functions
to be sampled on the space $\Omega$.
Let $\{\varrho_{{i}_1,...,i_k}\}(k\leq N,i_j\in\{1,2,..,N\})$
be combined probability density functions of the combined classes.
$\varrho_{{i}_1,...,i_k}$ is the combined density function of the class combined by  the $i_1th,...,i_kth$ classes,
%A combined probability density function $\rho_{i_1,i_2,...,i_k}$,
% which describes distribution of m
% can be generated by $k(k\leq m)$ different probability density functions $\rho_{i_1},\rho_{i_2},...,\rho_{i_k}$,
% that is
\begin{equation*}
    \varrho_{i_1,i_2,...,i_k}=(\varrho_{i_1}+\varrho_{i_2}+...,+\varrho_{i_k})/k
\end{equation*}
Let $X_i$ is sampling points of the probability density function $\varrho_i$,
\begin{equation*}
  X_i=\{x_i^1,x_i^2,...,x_i^{n_i}\}\quad(1\leq i\leq N), x_i^j\in\Omega(1\leq j\leq n_i),
\end{equation*}
where $n_i$ is the number of sampling points $X_i$.

In terms of Eq.~\ref{probability-measure},
there is a corresponding probability measure $\nu_i(1\leq i\leq 2^N-1)$ for every $\varrho_i$ or  $\varrho_{{i_1,i_2,...,i_k}}$.
The probability measure $\nu_i(1\leq i\leq N)$ corresponds to $\varrho_i$,
and the probability measure $\nu_i(N<i\leq 2^N-1)$ corresponds to $\varrho_{i_1,...,i_k}$.
For sampling points $X_i$ or $\{X_{i_1}, X_{i_2},...,X_{i_k}\}$,
there is a corresponding discrete probability measure $\mu_i(1\leq i\leq 2^m-1)$.
For multi-class blue noise sampling,
every class sampling points $X_i$ captures the probability function $\varrho_i$ well,
and combined sampling points $\{X_{i_1}, X_{i_2},...,X_{i_k}\}$ captures the probability density function $\varrho_{i_1,i_2,...,i_k}$ well too.
Thus,
every discrete probability measure $\mu_i$ should approximate corresponding probability measure $\nu_i$ well.

Compared with a single class sampling,
we cannot compute an optimal $\mu_i$ for every $\nu_i$ in terms of Eq.~\ref{single-class-problem} independently,
since sampling points $X_i$ are influenced by individual probability measures $\nu_i$ and combined probability measuress $\nu_{i_1,...,i_k}(i_j=i)$ simultaneously.
We describe multi-class sampling as a combined Wasserstein barycenter,
\begin{equation}\label{eq:combined-wasserstein-barycenter}
 \bar\mu=arg\min\limits_{\bar\mu}\sum\limits_{i=1}^K\lambda_iW_p^p(\mu_i,\nu_i) \quad
  s.t.\sum\limits_{i=1}^K\lambda_i=1,(\lambda_i\geq0)
\end{equation}
where $\bar\mu=\{\mu_1,...,\mu_N,\mu_{N+1},...\mu_{K}\}$,
$\mu_i(1\leq i\leq N)$  is a discrete probability measure corresponding to $X_i$,
$\mu_i(N < i\leq K)$ is a discrete probability measure corresponding to combined sampling points.

In extended version of~\cite{wei:2010:multi},
Wei pointed out that multi-class sampling on traditional relaxation method~\cite{balzer:2009:capacity}
 might result in insufficient sample
uniformity for the reason below: for a given sample $s$,
different class combinations may have different opinions about the desired centroid
location to which $s$ should move.
In essence,
conflicts of desired centroid locations are introduced by the hard constraint on transport plans $\pi$ (Eq.~\ref{eq:transform-plan}).
In traditional relaxation method~\cite{balzer:2009:capacity},
the mass in a certain domain can only be transported to the nearest sampling point.
Thus, the transport plan is local,
and the minimum unit of transported mass is constrained by the resolution of the domain.
The two limits make relaxing conflicts of desired centroid locations more difficult.

To preserve sufficient sample uniformity for multi-class sampling,
the constrain on the transport plan (Eq.~\ref{eq:transform-plan})
is broken via a more general transport plan $\pi$ in combined Wasserstein barycenter Eq.~\ref{eq:combined-wasserstein-barycenter}.
On one hand,
$\pi$ is a separable transport plan in Eq.~\ref{eq:combined-wasserstein-barycenter},
which means the mass located in a small domain can be transported to different samples other than only one sampling point.
It extends the minimum unit of transported mass to computational accuracy.
Furthermore,
we relax transport plans via entropic regular terms as~\cite{cuturi:2013:sinkhorn}.
Regularized Wasserstein distance is defined as
\begin{equation}\label{eq:regulate-wasserstein-distance}
 W_p(\mu,\nu)=\left(\inf\limits_{\pi\in\prod(\mu,\nu)}\int_{\Omega^2}d(x,y)^pd\pi(x,y)+\epsilon H(\pi)\right )^{1/p}
\end{equation}
where $\epsilon$ is a positive regularization parameter,
and $H(\pi)$ is the entropy of $\pi$.
Entropic regularization terms expand transport range to whole domain other than nearest transport in traditional relaxation method~\cite{balzer:2009:capacity}.
It provides more flexibility for reducing conflicts of desired centroid.
Compared with multi-class sampling~\cite{wei:2010:multi},
our approach is an efficient relaxation method for multi-class blue noise sampling.

% other than a local transport plan.
%It make it possible to reduce the conflict in a more wide domain.
%In combined Wasserstein barycenter Eq.~\ref{eq:combined-wasserstein-barycenter},
%the constrain is broken via a general transport plan $\pi$.
%First,
%$\pi$ is a global transport plan other than a local transport plan,
%which is constrained in a local partition,
%such as discrete points~\cite{balzer:2009:capacity},
%Voronoi diagram~\cite{chen:2012:variational} or Power diagram~\cite{de:2012:blue}.
%Second,
%$\pi$ is a separable transport plan,
%which means the mass located in a small region can be transported to different samples other than only one sampling point.
%Furthermore,
%we relax transport plans via entropic regular terms as~\cite{cuturi:2013:sinkhorn}.

%More details can be found in~\cite{cuturi:2013:sinkhorn}.

%\subsection{Numerical Optimization}



\begin{figure}[htb]
\rule{0.5\textwidth}{0.4pt}
 \quad 01: //Wasserstein blue noise sampling\\
\quad 02: Input: domain $\Omega$, densities $\varrho_1,...,\varrho_N$, coordinates of discrete points $\mathbf{Y}$,
  number of sampling points $n_1,...,n_N$,
  number of combined classes $K-N$ and combined classes $\{i_1,...,i_k\}$\\
\quad 03: Output: coordinates $\mathbf{X}=\{x_{1,1} \cdots  x_{1,N_1} \cdots x_{n,1} \cdots x_{n,N_n}\}$  of sampled points\\
\quad 04: Initialize $\mathbf{X}$ with random points inside $\Omega$ conforming to $\rho_1,...,\rho_n$ \\
\quad 05: Initialize parameters $\epsilon$ for the entropic regularization\\
\quad 06: Initialize weight parameters $\lambda_i(1\leq i \leq K)$\\
%\quad 6: Initialize the parameter $t$ for the simulated annealing strategy\\
\quad 07: Initialize transport cost matrixes $\Pi_k=\frac{1}{nm}\mathbf{1}$, $\mathbf{1}$ is an all-1 matrix\\
\quad 08: Compute distance matrix $\mathbf D$\\
\quad 09: Compute the energy $E_0=\sum_{i=1}^K\lambda_i<\mathbf D,\mathbf{\Pi}_i>$\\
\quad 10: \textbf{Repeat}\\
11:\quad Compute the distance matrix $\mathbf{D}$ \\
12:\quad // Iterative bregman projections for $\mathbf{\Pi}_k$\\
13:\quad For i=1:K \\
14:\quad \quad  Iterative-bregman-projection() (lines 24-32) \\
15:\quad // Newton iterative method for $\mathbf{X}$ \\
16:\quad Generate K random numbers $r_1,...,r_K\in[0,1]$\\
17:\quad Normalize $r_1,...,r_K$,$\sum_{i=1}^Kr_i=1$\\
18:\quad Update $\mathbf X$, $\mathbf X=(1-\theta)\mathbf X+\theta\sum_{i=1}^K\lambda_ir_i\mathbf{X}\mathbf{\Pi}ik^Tdiag(\frac{1}{g_i})$\\
19:\quad Update $D$\\
20:\quad $E_1=\sum_{i=1}^K\lambda_i<\mathbf{D},\mathbf{\Pi}_k>$ \\
21:\quad $dE=E_1-E_0$, $E_0=E_1$\\
22:\quad Update $\epsilon$ \\
23: \textbf{Until} $dE$ is convergent \\

24: Subroutine Iterative bregman projections()\\
25: Input: the distance matrix $D$, the parameter $\epsilon$,
    the density function $\mathbf{p}=\mathbf{g}_k$,
    $\mathbf{q}=\mathbf{\varrho_{k}}$\\
26: Initiate $i=0$, $\mathbf{v}^i=\mathbf{1}$(all-1 vector)\\
27: Initiate $\mathbf\xi=e^{-\mathbf{D}/\epsilon}$\\
28: \textbf{Repeat}\\
29:\quad $\mathbf{u}^i=\frac{\mathbf{p}}{\mathbf{\xi}\mathbf{v}^i}$\\
30:\quad $i=i+1$,$\mathbf{v}^i=\frac{\mathbf{q}}{\mathbf{\xi}^\mathbf{T}\mathbf{u}^i}$\\
31:\textbf{Until} $\mathbf{\mu}^i$ is convergent\\
32:$\mathbf{\Pi}_k=diag(\mathbf{u}^i)\mathbf{\xi} diag({\mathbf{v}^i})$\\
   \rule{0.47\textwidth}{0.5pt}
  \caption{Pseudocode of multi-class blue noise algorithm}\label{fig:pseudocode}
\end{figure}


\subsection{Numerical Optimization}
Eq.~\ref{eq:combined-wasserstein-barycenter} is a unified variational model for single class sampling and multi-class sampling.
We only consider numerical optimization of Eq.~\ref{eq:combined-wasserstein-barycenter}
for multi-class sampling.
For every discrete probability measure $\mu_i\in\bar\mu$,
we represent it as:
\begin{equation}
 \mu_i=\sum\limits_{j=1}^{n_i}\rho_i^j\delta_{x_i^j} \quad s.t. \sum\limits_{j=1}^{n_i}\rho_i^j=1.
\end{equation}
where $\rho_i^j$ is probability at sampling point $x_i^j$ for the $i$th-class sampling
, $n_i$ is the number of sampling points for the $i$th-class sampling.
For convenience of representation and computation,
we extend discrete probability measure $\mu_i$ to all class sampling points.
We define all sampling points as
\begin{equation}
 X=(X_1,X_2,...,X_N)^T=(x_1^1,...,x_1^{n_1},...,x_N^1,...,x_N^{n_N})^T.
\end{equation}
\begin{equation}
\mathbf{g_i}=(\underbrace{0,0,...,0}_{\sum\limits_{j=1}^{i-1}n_j},\rho_i^1,\rho_i^2,...,\rho_i^{n_i},\underbrace{0,0,...,0}_{\sum\limits_{j=i+1}^{N}n_j})^T
\end{equation}
Thus,
we can represent discrete probability measure $\mu_i$ as:
\begin{equation}
  \mu_i=\sum\limits_{j=1}^{n}g_i^j\delta_{x^j}
\end{equation}
where $n=\sum\limits_{i=1}{n_i}$.
For blue noise sampling,
$\rho_i^j$ is a constant and $\rho_i^j=1/n_i$.
Thus, we need only to compute variables $X$ in Eq. ~\ref{eq:combined-wasserstein-barycenter} .

Every probability measure $\nu_i$ in Eq.~\ref{eq:combined-wasserstein-barycenter} may be a continuous probability measure or a discrete probability measure.
If $\nu_i$ is a continuous probability measure,
we must discretize  $\nu_i$ as a discrete probability measure
to compute integrations required in Eq. ~\ref{eq:combined-wasserstein-barycenter} by quadrature,
\begin{equation}\label{eq:discrete-prbability-measure}
\nu_i=\sum\limits_{j=1}^{m_i}\varrho_i^j\delta_{y_i^j} \quad s.t. \sum\limits_{j=1}^{m_i}\varrho_i^j=1
\end{equation}
where $\varrho_i^j=\varrho_k(y_i^j)V_{y_i^j}$
and $V_{y_i^j}$ is the area with density $\varrho_k(y_i^j)$ at the point $y_i^j$ when the density function
$\varrho_i$ is approximated with a positive piecewise constant function,
$\varrho_i$ is the probability density function of the probability measure $nu_i$,
and $m_i$ is the number of discrete points of the $i$th-class probability measure $\nu_i$.
In this paper,
we apply a regular discretization with same area for all sampling points $y_i^j$.
We also apply the same sampling points for all probability measures $\nu_i$, 
which is
\begin{equation*}
m_i=m_k=m, y_i^j=y_k^j=y^j \quad{i\neq j}.
\end{equation*}
Thus, we use $\mathbf{Y}=(y^1,y^2,...,y^m)^T$ to represent all sampling points for probability measures $\nu_i$.
Based on discrete representation of probability measures in Eq.~\ref{eq:discrete-prbability-measure},
%and regularized Wasserstein distance (Eq.~\ref{eq:regulate-wasserstein-distance}),
variational problem (Eq.~\ref{eq:combined-wasserstein-barycenter}) can be represented as:
\begin{equation}\label{eq:discrete-multi-problem}
  \begin{aligned}
  \mathbf{X}=\arg\min\limits_{\mathbf{X}}\sum\limits_{i=1}^K\lambda_i<\mathbf{D}_i,\mathbf{\Pi}_i> \\
    s.t.\ \ \mathbf{\Pi}_i\mathbf{1}=\mathbf{\varrho}_i,\ \mathbf{\Pi}_i^T\mathbf{1}=\mathbf{g}_i
  \end{aligned}
\end{equation}
where
% $\mathbf{X}=\{X_1,...,X_N\}$ is the set of all samples,
 $\mathbf{D}_i,\mathbf{\Pi}_i\in R_+^{n\times m}$,
 $\mathbf{D}_i$ is the distance matrix and $D_i^{(j,k)}=d(x^j,y^k)^p$,
 $\mathbf{\Pi}_i$ is the $k$th-class transport cost matrix and $\Pi_i(x^j,y^k)$ represents the mount of mass transported from $x^j$ to $y^k$ for the $i$-th class,
 and $<\cdot,\cdot>$ is Frobenius product of two matrixes.

 We proceed with sample generation by computing a optimal solution of the variational problem (Eq.\ref{eq:discrete-multi-problem}).
 We apply a loop iteration algorithm to extremize the variational problem by repeatedly performing a minimization step over positions $\mathbf{X}$ followed by a projection step over transport plans $\mathbf{\Pi}_k$.

 For a fixed set of points $\mathbf X$,
 we compute  relaxed transport plans via regularized Wasserstein distance (Eq.\ref{eq:regulate-wasserstein-distance}).
 For every transport plan matrix $\mathbf{\Pi}_k$,
 Eq.\ref{eq:regulate-wasserstein-distance} can be discretized as
 \begin{equation}\label{eq:discrete-regulate-wasserstein-barycenter}
   W_p^p(\mathbf{D}_k,\mathbf{\Pi}_k)=arg\min\limits_{\mathbf{\Pi}_k}<\mathbf{D}_k,\mathbf{\Pi}_k>-\epsilon H(\mathbf{\Pi}_k)
 \end{equation}
The optimal transport plan $\mathbf{\Pi}_k$ can be obtained by iterative Bregman projection~\cite{cuturi:2013:sinkhorn,benamou:2015:iterative}, and
\begin{equation}\label{eq:transport-plan}
  \mathbf{\Pi}_k=diag(\mathbf{u})\mathbf{\xi}diag(\mathbf{v})
\end{equation}
where $\mathbf{\xi}=e^{-\frac{\mathbf{D}_k}{\epsilon}}$,
$\mathbf{u}\in \mathbb{R}_+^{n_k}$ and $\mathbf{v}\in\mathbb{R}_+^{m_k}$.
More details can be found in~\cite{cuturi:2013:sinkhorn,cuturi:2013:fast,benamou:2015:iterative}



Suppose $\Pi_k(k=1,...,K)$ is fixed,
variational formula Eq.~\ref{eq:discrete-multi-problem}
is the sum of a convex quadratic function of $\mathbf{X}$ when $p=2$.
If we only update $\mathbf{X}_i$ at every iteration,
$\mathbf{X}$ can be obtained by Newton iterative method,
\begin{equation}\label{eq:position-iterative}
  \mathbf{X}\leftarrow(1-\theta)\mathbf{X}+\theta\mathbf{Y}\mathbf{\Pi}_kdiag({\rho_k}^{-1}) \quad (0\leq\theta\leq 1)
\end{equation}
To improve randomness of samples,
we apply a random Newton iterative method to update all sampling points $\mathbf{X}$ simultaneously,
\begin{equation}
\begin{split}
\mathbf{X}\leftarrow(1-\theta)\bar X+\theta\sum_{i=1}^K\lambda_ir_iX\Pi_k^Tdiag(\frac{1}{n_i})\\
 s.t. \sum\limits_{i=1}^{K}r_i=1, r_i\geq 0
 \end{split}
\end{equation}

where $r_i$ is a random number.
Figure~\ref{fig:pseudocode}
shows the pseudocode of this schedule.

%To avoid the algorithm to stuck in local minima,
%a simulated annealing strategy is introduced to the gradient descent method.

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}\textwidth
  \subfigure[{Dart throwing [Wei 2010]}]{

  \includegraphics[width=1.0\textwidth]{figure/3-class-weiliyi.png}}
  \subfigure[Our method]{

  \includegraphics[width=1.0\textwidth]{figure/3-class-our.png}}
  \caption{The comparison of Wei's algorithm and our algorithm for three-class blue noise sampling.
  $\lambda_{1,2,3,4,5,6,7}=1/18,1/18,1/18,2/18,2/18,2/14,9/18$.
  $\lambda_1$,$\lambda_2$ and $\lambda_3$ are weighted parameters for each individual class.
  $\lambda_1$,$\lambda_2$ and $\lambda_3$ are weighted parameters for combined classes with two individual class.
  $\lambda_8$ is the weighted parameters for the total samples.
  The number of samples of each individual class is $1024$.}
  \label{three-class-sampling}
\end{figure*}











