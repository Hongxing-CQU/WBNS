\section{Algorithm}

In the paper,
we take single class blue noise samples  as the Wasserstein barycenter of a probability measure.
Multi-class blue noise samples are chosen as the combined Wasserstein barycenter of multiple probability measures.
\subsection{Single class blue noise sampling}
Given an arbitrary space $\Omega$ and a density function $\varrho(x)$ on the space $\Omega$,
a probability measure is defined as:
\begin{equation}\label{probability-measure}
  \begin{split}
  \nu(U)=\int_U\varrho(x)dx, U\subseteq\Omega \\
  s.t. \int_{\Omega}\varrho(x)dx=1,\varrho(x)\geq0
  \end{split}
\end{equation}
Sampling the density function $\varrho(x)$ consists of picking a few representative points $x_i$
that capture $\varrho$ well.
These representative points $x_i$ represent a discrete probability measure $\mu$,
\begin{equation*}
\begin{split}
  \mu=\sum\limits_{i=1}^n\rho_i\delta_{x_i} \quad
  s.t. \sum\limits_{i=1}^n\rho_i=1,  \delta_{x_i}= \left\{ \begin{array}{ll}
  1 & x_i\in\Omega\\
  0 & \textrm{others}
  \end{array} \right.
  \end{split}
\end{equation*}
where $n$ is the number of samples and
$\rho_i$ is the probability at $x_i$.
Sampling, therefore, means that $\mu$ captures $\nu$ well.
We can use the Wasserstein barycenter to model a single class sampling as:
\begin{equation}\label{single-class-problem}
\begin{split}
 \mu=arg\min\limits_\mu W_p^p(\mu,\nu)=\inf\limits_\mu\int_{X\times\Omega}d(x_i,y)^pd\pi(x_i,y)\\
 s.t. \int_\Omega\pi(x_i,x)=\rho_i, \sum\limits_{i=1}^n\int_{U\subseteq\Omega}\pi(x_i,y)dy=\nu(U)
 \end{split}
\end{equation}
where $X=\{x_1,x_2,...,x_n\}\subseteq\Omega$.

Compared with previous methods on CCVT~\cite{balzer:2009:capacity,de:2012:blue},
the Wasserstein barycenter provides a more general framework for sampling.
Previous methods on CCVT are special cases of Eq.~\ref{single-class-problem} with an implicit constraint on  transport plan $\pi$,
that is
\begin{equation}\label{eq:transform-plan}
  \pi(x_i,y)\cdot\pi(x_j,y)=0 \quad(i\neq j).
\end{equation}
Eq.~\ref{eq:transform-plan} means that the mass in a certain domain can only be transported to one sampling point.
For example,
the corresponding domain of sampling points consists of a few discrete points in~\cite{balzer:2009:capacity},
and the power diagram region of sampling points in~\cite{de:2012:blue}.
In fact,
Eq.~\ref{eq:transform-plan} is a very strict condition for the variational problem in Eq.~\ref{single-class-problem}.
It can only be satisfied when $\nu$ is a continuous probability measure.
It also introduces a "failure case" for multi-class sampling~\cite{wei:2010:multi}.
We will discuss the reason in the next subsection.

\subsection{Multi-class blue noise sampling}
%For multi-class sampling,
%we will compute $N$-class sampling points for $m$-class probability density functions,
%where $N$ is the number of individual probability density functions and
%$m-N(m>N)$ is the number of combined probability density functions.
For multi-class blue noise sampling,
both each individual class and combined classes exhibit blue noise characteristics~\cite{wei:2010:multi}.
It is necessary to consider individual probability distributions and combined probability distributions simultaneously
in multi-class sampling. \\
%sampling points are drawn from $N$-class individual probability distribution
%and $M$-class combined probability distribution.
Let $\{\varrho_1,\cdots,\varrho_N\}$ be $N$ individual probability density functions
to be sampled on the space $\Omega$.
Let $\{\varrho_{{i}_1,...,i_k}\}(k\leq N, i\in\{N+1,N+2...,2^N-1\},i_j\in\{1,2,..,N\})$
be combined probability density functions of the combined classes,
which is combined by  the $i_1th,...,i_kth$ classes,
%A combined probability density function $\rho_{i_1,i_2,...,i_k}$,
% which describes distribution of m
% can be generated by $k(k\leq m)$ different probability density functions $\rho_{i_1},\rho_{i_2},...,\rho_{i_k}$,
% that is
\begin{equation*}
    \varrho_{i_1,i_2,...,i_k}=(\varrho_{i_1}+\varrho_{i_2}+...,+\varrho_{i_k})/k
\end{equation*}
Let $X_i$ be sampling points of the probability density function $\varrho_i$,
\begin{equation*}
  X_i=\{x_i^1,x_i^2,...,x_i^{n_i}\}\quad(1\leq i\leq N), x_i^j\in\Omega(1\leq j\leq n_i),
\end{equation*}
where $n_i$ is the number of sampling points $X_i$.

In terms of Eq.~\ref{probability-measure},
there is a corresponding probability measure $\nu_i(1\leq i\leq 2^N-1)$ for every $\varrho_i$ or  $\varrho_{{i_1,i_2,...,i_k}}$.
The probability measure $\nu_i(1\leq i\leq N)$ corresponds to $\varrho_i$,
and the probability measure $\nu_i(N<i\leq 2^N-1)$ corresponds to $\varrho_{i_1,...,i_k}$.
For sampling points $X_i$ or $\{X_{i_1}, X_{i_2},...,X_{i_k}\}$,
there is a corresponding discrete probability measure $\mu_i(1\leq i\leq 2^N-1)$.
For multi-class blue noise sampling,
every class sampling point $X_i$ captures the probability function $\varrho_i$ well,
and combined sampling points $\{X_{i_1}, X_{i_2},...,X_{i_k}\}$ also capture the probability density function $\varrho_{i_1,i_2,...,i_k}$ well.
Thus,
every discrete probability measure $\mu_i$ should approximate corresponding probability measure $\nu_i$ well.

Compared with a single class sampling,
we cannot compute an optimal $\mu_i$ for every $\nu_i$ in terms of Eq.~\ref{single-class-problem} independently,
since sampling points $X_i$ are influenced by individual probability measures $\nu_i$ and combined probability measuress $\nu_{i'}(i'_j=i)$ simultaneously.
We describe multi-class sampling as a combined Wasserstein barycenter,
\begin{equation}\label{eq:combined-wasserstein-barycenter}
 \bar\mu=arg\min\limits_{\bar\mu}\sum\limits_{i=1}^K\lambda_iW_p^p(\mu_i,\nu_i) \quad
  s.t.\sum\limits_{i=1}^K\lambda_i=1,(\lambda_i\geq0)
\end{equation}
where $\bar\mu=\{\mu_1,...,\mu_N,\mu_{N+1},...\mu_{K}\}$,
$\mu_i(1\leq i\leq N)$  is a discrete probability measure corresponding to $X_i$,
$\mu_i(N < i\leq K)$ is a discrete probability measure corresponding to combined sampling points and
$K-N$ is the number of combined classes.

In an extended version of~\cite{wei:2010:multi},
Wei pointed out that multi-class sampling on a traditional relaxation method~\cite{balzer:2009:capacity}
 might result in insufficient sample
uniformity for the reason below: for a given sample $s$,
different class combinations may have different opinions about the desired centroid
location to which $s$ should move.
In essence,
conflicts of desired centroid locations are introduced by the hard constraint on transport plans $\pi$ (Eq.~\ref{eq:transform-plan}).
In the traditional relaxation method~\cite{balzer:2009:capacity},
the mass in a certain domain can only be transported to the nearest sampling point.
Thus, the transport plan is local,
and the minimum unit of transported mass is constrained by the resolution of the domain.
The two limits make relaxing conflicts of desired centroid locations more difficult.

In order to preserve sufficient sample uniformity for multi-class sampling,
the constraint on the transport plan (Eq.~\ref{eq:transform-plan})
is broken through a more general transport plan $\pi$ in combined with a Wasserstein barycenter (Eq.~\ref{eq:combined-wasserstein-barycenter}).
On one hand,
$\pi$ is a separable transport plan in Eq.~\ref{eq:combined-wasserstein-barycenter},
which means the mass, located in a small domain, can be transported to different samples other than only one sampling point.
It extends the minimum unit of transported mass to computational accuracy.
Furthermore,
we relax transport plans via entropic regular terms as~\cite{cuturi:2013:sinkhorn}.
Regularized Wasserstein distance is defined as
\begin{equation}\label{eq:regulate-wasserstein-distance}
 W_p(\mu,\nu)=\left(\inf\limits_{\pi\in\prod(\mu,\nu)}\int_{\Omega^2}d(x,y)^pd\pi(x,y)+\epsilon H(\pi)\right )^{1/p}
\end{equation}
where $\epsilon$ is a positive regularization parameter,
and $H(\pi)$ is the entropy of $\pi$.
Entropic regularization terms expand transport range to the whole domain rather than the nearest transport in traditional relaxation method~\cite{balzer:2009:capacity}.
It provides more flexibility for reducing conflicts of the desired centroid.
Compared with multi-class sampling~\cite{wei:2010:multi},
our approach is an efficient relaxation method for multi-class blue noise sampling.

% other than a local transport plan.
%It make it possible to reduce the conflict in a more wide domain.
%In combined Wasserstein barycenter Eq.~\ref{eq:combined-wasserstein-barycenter},
%the constrain is broken via a general transport plan $\pi$.
%First,
%$\pi$ is a global transport plan other than a local transport plan,
%which is constrained in a local partition,
%such as discrete points~\cite{balzer:2009:capacity},
%Voronoi diagram~\cite{chen:2012:variational} or Power diagram~\cite{de:2012:blue}.
%Second,
%$\pi$ is a separable transport plan,
%which means the mass located in a small region can be transported to different samples other than only one sampling point.
%Furthermore,
%we relax transport plans via entropic regular terms as~\cite{cuturi:2013:sinkhorn}.

%More details can be found in~\cite{cuturi:2013:sinkhorn}.

%\subsection{Numerical Optimization}



\begin{figure}[htb]
\rule{0.5\textwidth}{0.4pt}
 \quad 01: //Wasserstein blue noise sampling\\
\quad 02: Input: domain $\Omega$, densities $\varrho_1,...,\varrho_N$, coordinates of discrete points $\mathbf{Y}$,
  number of sampling points $n_1,...,n_N$,
  number of combined classes $K-N$ and combined classes $\{i_1,...,i_k\}$\\
\quad 03: Output: coordinates $\mathbf{X}=\{x_1^1 \cdots  x_1^{N_1} \cdots x_n^1 \cdots x_n^{N_n}\}$  of sampled points\\
\quad 04: Initialize $\mathbf{X}$ with random points inside $\Omega$ conforming to $\rho_1,...,\rho_n$ \\
\quad 05: Initialize parameters $\epsilon$ for the entropic regularization\\
\quad 06: Initialize weight parameters $\lambda_i(1\leq i \leq K)$\\
%\quad 6: Initialize the parameter $t$ for the simulated annealing strategy\\
\quad 07: Initialize transport cost matrixes $\Pi_k=\frac{1}{nm}\mathbf{1}$, $\mathbf{1}$ is an all-1 matrix\\
\quad 08: Compute distance matrix $\mathbf D$\\
\quad 09: Compute the energy $E_0=\sum_{i=1}^K\lambda_i<\mathbf D,\mathbf{\Pi}_i>$\\
\quad 10: \textbf{Repeat}\\
11:\quad // Iterative bregman projections for $\mathbf{\Pi}_k$\\
12:\quad For i=1:K \\
13:\quad \quad  Iterative-bregman-projection() (lines 24-32) \\
14:\quad // Newton iterative method for $\mathbf{X}$ \\
15:\quad Generate K random numbers $r_1,...,r_K\in[0,1]$\\
16:\quad Normalize $r_1,...,r_K$,$\sum_{i=1}^Kr_i=1$\\
17:\quad Update $\mathbf X$, $\mathbf X=(1-\theta)\mathbf X+\theta\sum_{i=1}^K\lambda_ir_i\mathbf{X}\mathbf{\Pi}i^Tdiag({\rho_i^{-1}})$\\
18:\quad Update $D$\\
19:\quad $E_1=\sum_{i=1}^K\lambda_i<\mathbf{D},\mathbf{\Pi}_i>$ \\
20:\quad $dE=E_1-E_0$, $E_0=E_1$\\
21:\quad Compute the distance matrix $\mathbf{D}$ \\
22:\quad Update $\epsilon$ \\
23: \textbf{Until} $dE$ is convergent \\

24: Subroutine Iterative bregman projections()\\
25: Input: the distance matrix $D$, the parameter $\epsilon$,
    the density function $\mathbf{p}=\rho_k$,
    $\mathbf{q}=\mathbf{\varrho_{k}}$\\
26: Initiate $i=0$, $\mathbf{v}^i=\mathbf{1}$(all-1 vector)\\
27: Initiate $\mathbf\xi=e^{-\mathbf{D}/\epsilon}$\\
28: \textbf{Repeat}\\
29:\quad $\mathbf{u}^i=\frac{\mathbf{p}}{\mathbf{\xi}_k\mathbf{v}^i}$\\
30:\quad $i=i+1$,$\mathbf{v}^i=\frac{\mathbf{q}}{\mathbf{\xi}_k^\mathbf{T}\mathbf{u}^i}$\\
31:\textbf{Until} $\mathbf{\mu}^i$ is convergent\\
32:$\mathbf{\Pi}_k=diag(\mathbf{u}^i)\mathbf{\xi}_k diag({\mathbf{v}^i})$\\
   \rule{0.47\textwidth}{0.5pt}
  \caption{Pseudocode of multi-class blue noise algorithm}\label{fig:pseudocode}
\end{figure}


\subsection{Numerical Optimization}
Eq.~\ref{eq:combined-wasserstein-barycenter} is a unified variational model for single class sampling and multi-class sampling.
We only consider numerical optimization of Eq.~\ref{eq:combined-wasserstein-barycenter}
for multi-class sampling.
For every discrete probability measure $\mu_i\in\bar\mu$,
we represent it as:
\begin{equation}
 \mu_i=\sum\limits_{j=1}^{n_i}\rho_i^j\delta_{x_i^j} \quad s.t. \sum\limits_{j=1}^{n_i}\rho_i^j=1.
\end{equation}
where $\rho_i^j$ is probability at sampling point $x_i^j$ for the $i$th-class sampling
and $n_i$ is the number of sampling points for the $i$th-class sampling.
For convenience of representation and computation,
we extend the discrete probability measure $\mu_i$ to all class sampling points.
We define all sampling points as
\begin{equation}
 X=(X_1,X_2,...,X_N)^T=(x_1^1,...,x_1^{n_1},...,x_N^1,...,x_N^{n_N})^T.
\end{equation}
\begin{equation}
\mathbf{g_i}=(\underbrace{0,0,...,0}_{\sum\limits_{j=1}^{i-1}n_j},\rho_i^1,\rho_i^2,...,\rho_i^{n_i},\underbrace{0,0,...,0}_{\sum\limits_{j=i+1}^{N}n_j})^T
\end{equation}
Thus,
we can represent the discrete probability measure $\mu_i$ as:
\begin{equation}
  \mu_i=\sum\limits_{j=1}^{n}g_i^j\delta_{x_i^j}
\end{equation}
where $n=\sum\limits_{i=1}^N{n_i}$.
For blue noise sampling,
$\rho_i^j$ is a constant and $\rho_i^j=1/n_i$.
Thus, we need only to compute variables $X$ in Eq. ~\ref{eq:combined-wasserstein-barycenter} .

\begin{figure}[ht]
\centering
\subfigure[]{
\label{one-class-a}
   \includegraphics[width=.4\textwidth]{figure/one-class-1024-1.png}
}
\subfigure[]{
\label{one-class-b}
   \includegraphics[width=.45\textwidth]{figure/remp.png}
}
\caption{
Comparisons of CCVT, CCVT-PD and our algorithm for a single class blue noise sampling.
(a) shows the comparison for the case of constant density.
(b) shows the comparison on for the case of a quadratic density function. }
\label{one-class-sampling}
\end{figure}



\begin{table}
\caption{\label{comparing-table}
Comparison of several frequencies and spatial statistics of sampling patterns.
}
 \begin{tabular}{c|c|c|c|c|c}

 % after \\: \hline or \cline{col1-col2} %\cline{col3-col4}
  \hline
   Methods & $v_{eff}$ & $\Omega$ & $\delta_{min}$ & $\delta_{avg}$ & $Q_6$ \\ \hline
  {[Balzer et al. 2009]}& 0.88& 2.395 & 0.765 & 0.886 & 0.488 \\
  {[De Goes et al. 2012]} & 0.855 & 2.014 & 0.739 & 0.872 & 0.417 \\
   Our algorithm & 0.865 & 2.136 & 0.76 & 0.877 & 0.42 \\ \hline
  \end{tabular}\\
  Notes:$v_{eff} $ and $\Omega$ are the effective Nyquist and oscillation measures.
The value of $\delta_{min}$ corresponds to the Poisson disk radius,
and $\delta_{avg}$ roughly measures how uniformly the points are distributed~\cite{schlomer:2011:farthest}.
$Q_6$ measures the similarity of a point distribution to a hexagonal arrangement~\cite{kansal:2000:nonequilibrium}
\end{table}


Every probability measure $\nu_i$ in Eq.~\ref{eq:combined-wasserstein-barycenter} may be a continuous probability measure or a discrete probability measure.
If $\nu_i$ is a continuous probability measure,
we must discretize  $\nu_i$ as a discrete probability measure
to compute the integrations required in Eq. ~\ref{eq:combined-wasserstein-barycenter} by quadrature,
\begin{equation}\label{eq:discrete-prbability-measure}
\nu_i=\sum\limits_{j=1}^{m_i}\varrho_i^j\delta_{y_i^j} \quad s.t. \sum\limits_{j=1}^{m_i}\varrho_i^j=1
\end{equation}
where $\varrho_i^j=\varrho_i(y_i^j)V_{y_i^j}$
and $V_{y_i^j}$ is the area with density $\varrho_i(y_i^j)$ at the point $y_i^j$ when the density function
$\varrho_i$ is approximated with a positive piecewise constant function,
$\varrho_i$ is the probability density function of the probability measure $\nu_i$,
and $m_i$ is the number of discrete points of the $i$th-class probability measure $\nu_i$.
In this paper,
we apply a regular discretization with the same area for all sampling points $y_i^j$.
We also apply the same sampling points for all probability measures $\nu_i$,
which is
\begin{equation*}
m_i=m_k=m, y_i^j=y_k^j=y^j \quad{i\neq k}.
\end{equation*}
Thus, we use $\mathbf{Y}=(y^1,y^2,...,y^m)^T$ to represent all sampling points for probability measures $\nu_i$.
Based on discrete representation of probability measures in Eq.~\ref{eq:discrete-prbability-measure},
%and regularized Wasserstein distance (Eq.~\ref{eq:regulate-wasserstein-distance}),
the variational problem (Eq.~\ref{eq:combined-wasserstein-barycenter}) can be represented as:
\begin{equation}\label{eq:discrete-multi-problem}
  \begin{aligned}
  \mathbf{X}=\arg\min\limits_{\mathbf{X}}\sum\limits_{i=1}^K\lambda_i<\mathbf{D}_i,\mathbf{\Pi}_i> \\
    s.t.\ \ \mathbf{\Pi}_i\mathbf{1}=\mathbf{\varrho}_i,\ \mathbf{\Pi}_i^T\mathbf{1}=\mathbf{g}_i
  \end{aligned}
\end{equation}
where
% $\mathbf{X}=\{X_1,...,X_N\}$ is the set of all samples,
 $\mathbf{D}_i,\mathbf{\Pi}_i\in R_+^{n\times m}$,
 $\mathbf{D}_i$ is the distance matrix and $D_i^{(j,k)}=d(x_i^j,y^k)^p$,
 $\mathbf{\Pi}_i$ is the $i$th-class transport cost matrix and $\Pi_i(x_i^j,y^k)$ represents the mount of mass transported from $x_i^j$ to $y^k$ for the $i$-th class,
 and $<\cdot,\cdot>$ is Frobenius product of two matrixes.

 We proceed with sample generation by computing an optimal solution of the variational problem (Eq.\ref{eq:discrete-multi-problem}).
 We apply a loop iteration algorithm to extremize the variational problem by repeatedly performing a minimization step over positions $\mathbf{X}$ followed by a projection step over transport plans $\mathbf{\Pi}_k$.

 For a fixed set of points $\mathbf X$,
 we compute  relaxed transport plans through a  regularized Wasserstein distance (Eq.\ref{eq:regulate-wasserstein-distance}).
 For every transport plan matrix $\mathbf{\Pi}_k$,
 Eq.\ref{eq:regulate-wasserstein-distance} can be discretized as
 \begin{equation}\label{eq:discrete-regulate-wasserstein-barycenter}
   W_p^p(\mathbf{D}_i,\mathbf{\Pi}_i)=arg\min\limits_{\mathbf{\Pi}_i}<\mathbf{D}_i,\mathbf{\Pi}_i>-\epsilon H(\mathbf{\Pi}_i)
 \end{equation}
The optimal transport plan $\mathbf{\Pi}_i$ can be obtained by iterative Bregman projection~\cite{cuturi:2013:sinkhorn,benamou:2015:iterative}, and
\begin{equation}\label{eq:transport-plan}
  \mathbf{\Pi}_i=diag(\mathbf{u})\mathbf{\xi}_idiag(\mathbf{v})
\end{equation}
where $\mathbf{\xi}_i=e^{-\frac{\mathbf{D}_i}{\epsilon}}$,
$\mathbf{u}\in \mathbb{R}_+^{n}$ and $\mathbf{v}\in\mathbb{R}_+^{m}$.
More details can be found in~\cite{cuturi:2013:sinkhorn,cuturi:2013:fast,benamou:2015:iterative}



Suppose $\Pi_i(i=1,...,K)$ is fixed,
variational formula Eq.~\ref{eq:discrete-multi-problem}
is the sum of a convex quadratic function of $\mathbf{X}$ when $p=2$.
If we only update $\mathbf{X}_i$ at every iteration,
$\mathbf{X}$ can be obtained by Newton iterative method,
\begin{equation}\label{eq:position-iterative}
  \mathbf{X}\leftarrow(1-\theta)\mathbf{X}+\theta\mathbf{Y}\mathbf{\Pi}_i^T diag({\rho_i}^{-1}) \quad (0\leq\theta\leq 1)
\end{equation}
To improve the randomness of samples,
we apply a random Newton iterative method to update all sampling points $\mathbf{X}$ simultaneously,
\begin{equation}
\begin{split}
\mathbf{X}\leftarrow(1-\theta)X+\theta\sum_{i=1}^K\lambda_ir_i\mathbf{Y}\Pi_i^Tdiag({\rho_i}^{-1})\\
 s.t. \sum\limits_{i=1}^{K}r_i=1, r_i\geq 0
 \end{split}
\end{equation}

where $r_i$ is a random number.
Figure~\ref{fig:pseudocode}
shows the pseudocode of this schedule.

%To avoid the algorithm to stuck in local minima,
%a simulated annealing strategy is introduced to the gradient descent method.
