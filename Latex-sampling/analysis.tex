\section{Results}

\begin{figure*}[htb]
  \centering
 \subfigure[{CCVT [Balzer et al. 2009]}]{
  \label{fig:eight:a}
  \includegraphics[width=0.48\textwidth]{figure/sampling-ratio-CCVT.png}}
   \subfigure[{Ours}]{
  \label{fig:eight:b}
  \includegraphics[width=0.48\textwidth]{figure/sampling-ratio-our.png}}

  \caption{Evaluation on sampling ratio.
  The number of sampling points is $1024$.
  From left to right, sampling ratios are $1/(4,8,16,32,64)$,
  and the number of sampling points is $1024$.
  When the sampling ratio is no more than $1/4$,
  blue noise property is preserved well in our approach.
    % 随着这个的增加，ccvt方法蓝噪特性越来越好，
  % 但对我们的而言，当采用比例小于等于1/4时候，采样比就影响不大
  }
  \label{sampling-ratio}
\end{figure*}
\begin{figure*}[htb]
\centering
\subfigure[{ Dart throwing [Wei 2010]}]{
   \includegraphics[width=0.38\textwidth]{figure/2-class-weiliyi.png}
   \label{two-class-sampling-a}
}%
\hspace{0.1in}
\subfigure[{ Ours: $\lambda_{1,2,3}=(1,1,1)/3$}]{
   \includegraphics[width=.38\textwidth]{figure/2-111.png}
   \label{two-class-sampling-a}
}\\
\subfigure[{ Ours: $\lambda_{1,2,3}=(1,1,2)/4$}]{
   \includegraphics[width=.38\textwidth]{figure/2-112.png}
   \label{two-class-sampling-c}
}%
\hspace{0.1in}
\subfigure[{ Ours: $\lambda_{1,2,3}=(1,1,6)/8$}]{
   \includegraphics[width=.38\textwidth]{figure/2-116.png}
}%
\caption{The comparison of Wei's algorithm and our algorithm for two-class blue noise sampling.
In our algorithm, $\lambda_1$,$\lambda_2$,$\lambda_3$ are the weighted parameters for red samples, blue samples and the combined samples.
The number of samples for each class is 1000.
The weighted parameter $\lambda_i$ makes an influence on the trade-off between good distribution of the samples belonging to each individual class and good distribution of total samples.
Large $\lambda_i$ preserves the blue noise property of the $i$-$th$ well.
}\label{two-class-sampling}
\end{figure*}

We ran our algorithm on a variety of inputs:
from constant density to stripe and point set surface.
Various illustrations based on regularity and spectral analysis are
used throughout the paper to allow easy evaluation of our algorithm and  demonstrate how they  are compared to previous works.
We use methods in~\cite{schlomer:2011:accurate} and~\cite{wei:2011:differential} to analyze the
spectrum properties of sample distributions in two dimensional space and two dimensional manifold separately.
For spatial uniformity,
the relative radius~\cite{Lagae:2008:CPDD} is used to analyze the spatial uniformity.



\textbf{Single class sampling.}
Blue noise point distribution for a constant density
shows a characteristic blue noise profile in spectral space
and a typical spatial arrangement.
In Fig.~\ref{one-class-sampling},
we compare our algorithm with CCVT~\cite{balzer:2009:capacity} and CCVT-PD~\cite{de:2012:blue} method for the case of constant density in spectral space.
We also provide evaluations of the spatial properties in Table~\ref{comparing-table}.
Furthermore,
we also make an evaluation of blue noise sampling for an intensity ramp in Fig.~\ref{one-class-sampling}
by counting the number of points for each
quarter of the ramp.
It can be shown that our algorithm is comparable to CCVT~\cite{balzer:2009:capacity} and CCVT-PD~\cite{de:2012:blue} for both the cases of constant density and the intensity ramp.

Since the involved numerical optimization is based on discrete representation of sampled density functions in our approach,
we evaluate the effect of sampling ratio ($n/m$) on blue noise property by comparing our approach with CCVT~\cite{balzer:2009:capacity} in Fig.~\ref{sampling-ratio}.
When the sampling ratio is equal to $1/4$,
sampling on CCVT~\cite{balzer:2009:capacity} doesn't show blue noise property well.
With the decreasing sampling ratio, blue noise property is shown more and more well.
However,
the sampling ratio doesn't make an important effect on our approach.% when the sampling ratio is no more than $1/4$.
The reason is that the relaxed transport plan makes more original points exert influence on one sampling point in our approach, other than only nearest original points play the role in CCVT~\cite{balzer:2009:capacity}.



\textbf{Multi-class sampling on constant density.}
To evaluate blue noise distribution for multi-class sampling,
we first analyse the simplest case where all classes have the constant density function and same number of samples.
Fig.~\ref{two-class-sampling}, Fig.~\ref{three-class-sampling},
and Fig.~\ref{seven-class-sampling} illustrate the result generated by our method, Wei's ~\shortcite{wei:2010:multi}, and SPH~\cite{jiang:2015:blue} on two-class, three-class, and seven-class sampling separately.
In two-class sampling (Fig.~\ref{two-class-sampling}),
we illustrate the influence of the weighted parameter $\lambda_i$ on the trade-off between good distribution of the samples belonging to each individual class and good distribution of total samples.
Large $\lambda_i$ preserves the blue noise property of the $i$-$th$ well.
Depending on which distribution is more important,
$\lambda_i$ can be chosen for different applications.
When $\lambda_1+\lambda_2=\lambda_3$,
the balance between good distribution of the samples belonging to each individual class and good distribution of total samples
is achieved in Fig.~\ref{two-class-sampling-c}.
Fig.~\ref{two-class-sampling-a} and Fig.~\ref{two-class-sampling-c},
show that our results are comparable to that of Wei's.
In three-class sampling (Fig.~\ref{three-class-sampling}),
we evaluate blue noise distribution of samples of different combined classes.
Compared with Wei's method~\cite{wei:2010:multi} and SPH~\cite{jiang:2015:blue},
 blue noise profile of the combined class with two individual class is improved in our method
while blue noise distribution of each individual class is preserved.
In seven-class sampling (Fig.~\ref{seven-class-sampling}),
blue noise profile  is still kept for samples of each individual class and total class.
It shows that our method is feasible for more numbers of class sampling.

% 做一个
To evaluate efficiency of relaxing conflicts of desired centroid locations in our approach,
we compare our approach with Wei's method~\cite{wei:2010:multi} on the
difference between centroid locations of voronoi diagrams for single class samples and multi-class samples in Fig.~\ref{conflict-evaluation}.
We apply conflicts ratio to evaluate the difference between desired centroid locations.
Conflicts ratio is defined as
\begin{equation*}
  R=\frac{\sum\limits_{i=1}^N\sum\limits_{j=1}^{n_i}||C_{x_i^j}^I-C_{x_i^j}^C||}{\sum\limits_{i=1}^N\sum\limits_{j=1}^{n_i}D(x_i^j)}
\end{equation*}
where $C_{x_i^j}^I$ and $C_{x_i^j}^C$ are centroid locations of voronoi diagrams of the sample ${x_i^j}$ for individual class samples and combined class samples,
,and $D(x_i^j)$ is the distance between the sample $x_i^j$ and its nearest neighbor sample,
$n$ is the number of samples.
Compared with Wei's approach~\shortcite{wei:2010:multi},
conflicts ratio in our approach is decreased approximately by $50\%$.
It shows that our approach avoids the failure case of
multi-class sampling on traditional Lloyd relaxation [Wei 2010],
and is an efficient relaxation method for multi-class sampling.

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}\textwidth
  \subfigure[{Dart throwing [Wei 2010]}]{

  \includegraphics[width=0.9\textwidth]{figure/3-weiliyi.png}}

  \subfigure[{SPH [Jiang et al. 2015]}]{
  \includegraphics[width=0.9\textwidth]{figure/3-sph.png}}
    \subfigure[Ours]{

  \includegraphics[width=0.9\textwidth]{figure/3-our.png}}
  \caption{The comparison of Wei's algorithm, SPH and our algorithm for three-class blue noise sampling.
  $\lambda_{1,2,3,4,5,6,7}=(1,1,1,2,2,2,9)/18$.
  $\lambda_1$,$\lambda_2$ and $\lambda_3$ are weighted parameters for each individual class.
  $\lambda_1$,$\lambda_2$ and $\lambda_3$ are weighted parameters for combined classes with two individual class.
  $\lambda_8$ is the weighted parameters for the total samples.
  The number of samples of each individual class is $1024$.}
  \label{three-class-sampling}
\end{figure*}

\begin{figure*}[]
  \centering
  % Requires \usepackage{graphicx}\textwidth
 % \subfigure[{Dart throwing [Wei 2010]}]{
 % \label{fig:eight:a}
 % \includegraphics[width=0.8\textwidth]{figure/7-class-weiliyi-1024.png}}
%  \subfigure[Our method]{
 %  \label{fig:eight:b}
  \includegraphics[width=1.0\textwidth]{figure/7-class-our-1024.png}

  \caption{Seven-class blue noise sampling on our algorithm.
  $\lambda_{1,2,3,4,5,6,7,8}=(1,1,1,1,1,1,1,7)/14$.
  $\lambda_1,\cdots,\lambda_7$ is weighted parameters for each individual class,
  $\lambda_8$ is the weighted parameter for the total set.
  The number of samples of each individual class is $1024$.}\label{seven-class-sampling}
\end{figure*}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\textwidth]{figure/conflict-ratio.png}
    \caption{The evaluation for conflicts of desired voronoi centroid locations of individual classes and combined class.
    }
    \label{conflict-evaluation}
\end{figure}

\begin{figure}[htb]
  \centering
 \subfigure[Density function]{
  \label{fig:eight:a}
  \includegraphics[width=0.4\textwidth]{figure/adapt-color.png}}
   \subfigure[{Dart throwing [Wei 2010]}]{
  \label{fig:eight:b}
  \includegraphics[width=0.4\textwidth]{figure/adapt-liyiwei.png}}
   \subfigure[Ours]{
  \label{fig:eight:c}
  \includegraphics[width=0.4\textwidth]{figure/adapt-our.png}}
  \caption{Adaptive two-class sampling of a non-uniform density function with 1000 points for each class.
  The percentages in each quarter indicate ink density of different color in the image. In contrast,
  our results show precise adaptation for every single sampling and total sampling.}\label{adaptive sampling}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.45\textwidth]{figure/bunny-sampling.png}
  \caption{Two-class blue noise sampling on a point set surface.
  $\lambda_{1,2,3}=(1,1,2)/4$, and the number of samples belonging to each class is $2000$.  }\label{bunny-sampling}
\end{figure}


\textbf{Adaptive multi-class sampling on ramp.}
To evaluate multi-class blue noise sampling on non-uniform density function,
we applied two intensity ramp and count the number of points for each quarter of the ramp.
Fig.~\ref{adaptive sampling} shows samples generated by
our method and Wei's~\cite{wei:2010:multi}.
While samples of every individual class and the combined class represent approximately the right counting of points per quarter,
it can be seen that our results show noticeably less noise.

\textbf{Multi-class sampling on point set surface.}
Our method can be directly applied to multi-class sampling on point set surface.
We assume that original points represent a discrete probability measure $\nu=\sum\limits_{k=1}^{m}\varrho_k\delta_{y^k}$ as Eq.~\ref{eq:discrete-prbability-measure} on the surface.
$\varrho_k$ is set as the normalized area element at the point $y^k$ for multi-class blue noise sampling with constant density function.
$D_i^{(j,k)}$ is the geodesic distance.
For convenience,
we applied the Euclidean distance instead of the geodesic distance in this paper.
Samples are initialized on the surface.
After each iteration,
every sample is mapped back onto the surface by
moving least square projection~\cite{alexa:2001:point}.
In Fig.~\ref{bunny-sampling},
we make a two-class blue noise sampling on a bunny model,
and show the Differential Domain Function~\cite{wei:2011:differential} to demonstrate blue noise profile of different class sample distributions.

\textbf{Multi-class blue noise sampling with different sizes.}
Analogous to setting different radius of Possion disks for different class in dart throwing,
different sizes of samples can be set  via discrete probability measure $\mu_i$ for different classes in our method.
Suppose that the same constant density function $\varrho_i (1\leq i\leq N)$ is used for different classes,
we set the discrete probability density as $\rho_i^j=1/n_i$ for every individual class and $\rho_i^j=1/Nn_i$ for every combined class,
where $N$ is the number of individual classes of the combined class.
When two-class sampling is done with $N_1$ and $N_2$ samples ($N_1>N_2$) ,
every sample belonging to the $2$-th class is given more measure than every sample belonging to the $1$-th in the combined class.
When the density function of the combined class is a constant function,
the $2$-th sample is with larger size than the $1$-th sample.
The radius of every sample can be approximated as:
\begin{equation*}
  r_i=\lambda r_{i_{max}}/n
\end{equation*}
where $r_{i_max}$ is the maximun possible disk radius of the $i$-th class,
$\lambda$ is a relative radius parameter.
In our experiments,
we found that there is on overlap between any pair samples when $0\leq\lambda\leq 0.5$.
In Fig.~\ref{adaptive sampling},
our method is applied for three-class blue noise sampling with different size on a square and a two dimensional manifold.



\textbf{Complexity.}
One iteration of our algorithm involves iterative bregman projection.
The time complexity depends on what classes are involved in the framework.
For a single class sampling,
the time complexity is $O(MN)$.
Although this is worse than $O(NlogN+M)$,
iterative bregman projection can be easily parallelized on the GPU.
When only a total set is used as a combined class,
the time complexity is $O(2MN)$.
If all $2^n-1$ classes are involved,
the complexity is $O(2^{n-1}MN)$.
For a general application,
only a combined class of the total set is enough to generate a good point distribution.
Thus, it is significantly better than CapCVT~\cite{chen:2012:variational} which is $O(nMN+nN^2)$.
While our algorithm performs well for small datasets,
our method is limited by its memory requirements $O(2MN)$ for large datasets, such as the experiment in Fig.~\ref{Color stippling}.

\textbf{Performance.}
% 用的什么设备
% 针对单类做了什么，结论是什么
% 针对多类做了什么，结论是什么
All of our performance are locked on a workstation with Intel Xeon 3.50GHz
dual-core CPUs and 32GB memory, and NVIDIA Quadro K5000 GPU with 2GB memory.
For single blue noise sampling,
we compare our method with traditional CCVT~\cite{balzer:2009:capacity}
and power diagrams method~\cite{de:2012:blue} on running time in Table~\ref{time-table-singleclass}
Since the most time is spend for integral computation in power diagrams method~\cite{de:2012:blue} and distance matrix computation in our approach,
CCVT~\cite{balzer:2009:capacity} performs more better in running time than these two method when sampling rate is lower than $1/64$.
With the increasing of sampling points or the decreasing of sampling rate,
our approach outperforms CCVT~\cite{balzer:2009:capacity} and power diagrams~\cite{de:2012:blue}.
With the increasing of sampling points,
our approach is more and more faster than CCVT~\cite{balzer:2009:capacity},
and is two times faster than CCVT~\cite{balzer:2009:capacity} at least.
With the increasing of sampling points,
our approach is faster than power diagrams method~\cite{de:2012:blue} by over $10\%$.
For mutli-class blue noise sampling,
we compare our method with Wei's method~\cite{wei:2010:multi} on running time in Table~\ref{time-table-multiclass}.
Our approach is two times faster than Wei's method~\cite{wei:2010:multi} at least.
So far,
we use a global distance matrix in our algorithm.
More memory space is needed to store the matrix in our algorithm than other algorithm.
In every iteration,
updating the matrix spends most of running time.
Thus, our approach faces a challenge when numbers of original points and sampling points are large.
However, we also notice that the matrix is a sparse matrix in general.
Thus, we can furthermore improve the performance of our approach by making use of a sparse distance matrix.

\begin{table}
\center
\caption{\label{time-table-singleclass}
Comparison of sampling times of multi-class blue noise sampling.
Original points are generated from a constant density function.
The resolution of original points is $128\times 128$.
}

 \begin{tabular}{c|c|c|c|c}
 {Sampling} & {Sampling} &
 %\multirow{2}*{Original Points} &
 \multicolumn{3}{c}{ Times(s)} \\ \cline{3-5}
 \multirow{2}*{Points} & \multirow{2}*{Rate}
   & [Balzer et & [De Goes &  \multirow{2}* {Ours} \\
   &  & al. 2009] &  et al. 2012] & \\ \hline
 % after \\: \hline or \cline{col1-col2} %\cline{col3-col4}

   32 & 1/512&   0.203 & $0.325$ & 0.404 \\ \hline
   64 & 1/256& 0.187 & 0.407 & 0.346  \\ \hline
   128 & 1/128&  0.202 & 0.436 & 0.433  \\ \hline
   256 & 1/64& 0.375 & 0.568 & 0.474  \\ \hline
   512 & 1/32& 1.076 & 0.739 & 0.669  \\ \hline
   1024 & 1/16& 2.184 & 1.137 & 1.048  \\ \hline
   2048 & 1/8&  8.408 & 2.269 & 2.025  \\ \hline
   4096 & 1/4 & 30.841 & 4.406 & 4.005 \\ \hline

  \end{tabular}\\
\end{table}


\begin{table}
\center
\caption{\label{time-table-multiclass}
Comparison of sampling times of single blue noise sampling.
The number of sampling points of every individual class is 512.
The sampling rate is $1/16$ for every individual class sampling.
}

 \begin{tabular}{c|c|c}
 {Number of} &
 %\multirow{2}*{Original Points} &
 \multicolumn{2}{c}{ Times(s)} \\ \cline{2-3}
  Class & [Wei 2010] & Ours \\ \hline


 % after \\: \hline or \cline{col1-col2} %\cline{col3-col4}

   2 & 34.769 & 14.670  \\ \hline
   3 & 151.727 & 37.553   \\ \hline
   7 & 414.031 & 212.133 \\ \hline


  \end{tabular}\\
\end{table}
%We measure the timing via a uniform sampling.
%The number of all samples is 2048.
%The number of discrete points $M=128\times 128$.
%Only total set is considered as a combined class.
%Table. shows the computation times of our algorithm.
