\section{Results}

\begin{table}
\caption{\label{comparing-table}
Comparison of several frequencies and spatial statistics of sampling patterns.
}

 \begin{tabular}{c|c|c|c|c|c}

 % after \\: \hline or \cline{col1-col2} %\cline{col3-col4}
  \hline
   Methods & $v_{eff}$ & $\Omega$ & $\delta_{min}$ & $\delta_{avg}$ & $Q_6$ \\ \hline
  {[Balzer et al. 2009]}& 0.88& 2.395 & 0.765 & 0.886 & 0.488 \\
  {[De Goes et al. 2012]} & 0.855 & 2.014 & 0.739 & 0.872 & 0.417 \\
   Our algorithm & 0.865 & 2.136 & 0.76 & 0.877 & 0.42 \\ \hline
  \end{tabular}\\
  Notes:$v_{eff} $ and $\Omega$ are the effective Nyquist and oscillation measures.
The value of $\delta_{min}$ corresponds to the Poisson disk radius,
and $\delta_{avg}$ roughly measures how uniform of the points are distributed~\cite{schlomer:2011:farthest}.
$Q_6$ measures the similarity of a point distribution to a hexagonal arrangement~\cite{kansal:2000:nonequilibrium}
\end{table}



\begin{table}
\center
\caption{\label{time-table-singleclass}
Comparison of sampling times of multi-class blue noise sampling.
Original points are generated from a constant density function.
The resolution of original points is $128\times 128$.
}

 \begin{tabular}{c|c|c|c|c}
 {Sampling} & {Sampling} &
 %\multirow{2}*{Original Points} &
 \multicolumn{3}{c}{ Times(s)} \\ \cline{3-5}
 \multirow{2}*{Points} & \multirow{2}*{Rate}
   & [Balzer et & [De Goes &  \multirow{2}* {Ours} \\
   &  & al. 2009] &  et al. 2012] & \\ \hline



 % after \\: \hline or \cline{col1-col2} %\cline{col3-col4}

   32 & 1/512&   0.203 & $0.325$ & 0.404 \\ \hline
   64 & 1/256& 0.187 & 0.407 & 0.346  \\ \hline
   128 & 1/128&  0.202 & 0.436 & 0.433  \\ \hline
   256 & 1/64& 0.375 & 0.568 & 0.474  \\ \hline
   512 & 1/32& 1.076 & 0.739 & 0.669  \\ \hline
   1024 & 1/16& 2.184 & 1.137 & 1.048  \\ \hline
   2048 & 1/8&  8.408 & 2.269 & 2.025  \\ \hline

  \end{tabular}\\
\end{table}


\begin{table}
\center
\caption{\label{time-table-multiclass}
Comparison of sampling times of single blue noise sampling.
The number of sampling points of every individual class is 512.
The sampling rate is $1/16$ for every individual class sampling.
}

 \begin{tabular}{c|c|c}
 {Number of} &
 %\multirow{2}*{Original Points} &
 \multicolumn{2}{c}{ Times(s)} \\ \cline{2-3}
  Class & [Wei 2010] & Ours \\ \hline


 % after \\: \hline or \cline{col1-col2} %\cline{col3-col4}

   2 & 34.769 & 14.670  \\ \hline
   3 & 151.727 & 37.553   \\ \hline
   7 & 414.031 & 212.133 \\ \hline


  \end{tabular}\\
\end{table}

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}\textwidth
 % \subfigure[{Dart throwing [Wei 2010]}]{
 % \label{fig:eight:a}
 % \includegraphics[width=0.8\textwidth]{figure/7-class-weiliyi-1024.png}}
%  \subfigure[Our method]{
 %  \label{fig:eight:b}
  \includegraphics[width=1.0\textwidth]{figure/7-class-our-1024.png}

  \caption{Seven-class blue noise sampling on our algorithm.
  $\lambda_{1,2,3,4,5,6,7,8}=(1,1,1,1,1,1,1,7)/14$.
  $\lambda_1,\cdots,\lambda_7$ is weighted parameters for each individual class,
  $\lambda_8$ is the weighted parameter for the total set.
  The number of samples of each individual class is $1024$.}\label{seven-class-sampling}
\end{figure*}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\textwidth]{figure/conflict-ratio.png}
    \caption{The evaluation for conflicts of desired voronoi centroid locations of individual classes and combined class.
    }
    \label{conflict-evaluation}
\end{figure}

\begin{figure}[htb]
  \centering
 \subfigure[Density function]{
  \label{fig:eight:a}
  \includegraphics[width=0.4\textwidth]{figure/adapt-color.png}}
   \subfigure[{Dart throwing [Wei 2010]}]{
  \label{fig:eight:b}
  \includegraphics[width=0.4\textwidth]{figure/adapt-liyiwei.png}}
   \subfigure[Our algorithm]{
  \label{fig:eight:c}
  \includegraphics[width=0.4\textwidth]{figure/adapt-our.png}}
  \caption{Adaptive two-class sampling of a non-uniform density function with 1000 points for each class.
  The percentages in each quarter indicate ink density of different color in the image. In contrast,
  our results show precise adaptation for every single sampling and total sampling.}\label{adaptive sampling}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.45\textwidth]{figure/bunny-sampling.png}
  \caption{Two-class blue noise sampling on a point set surface.
  $\lambda_{1,2,3}=(1,1,2)/4$, and the number of samples belonging to each class is $2000$.  }\label{bunny-sampling}
\end{figure}
We ran our algorithm on a variety of inputs:
from constant density to stripe and point set surface.
Various illustrations based on regularity and spectral analysis are
used throughout the paper to allow easy evaluation of our algorithm and  demonstrate how they  are compared to previous works.
We use methods in~\cite{schlomer:2011:accurate} and~\cite{wei:2011:differential} to analyze the
spectrum properties of sample distributions in two dimensional space and two dimensional manifold separately.
For spatial uniformity,
the relative radius~\cite{Lagae:2008:CPDD} is used to analyze the spatial uniformity.

\textbf{Single class sampling.}
Blue noise point distribution for a constant density
shows a characteristic blue noise profile in spectral space
and a typical spatial arrangement.
In Fig.~\ref{one-class-sampling},
we compare our algorithm with CCVT~\cite{balzer:2009:capacity} and BOT~\cite{de:2012:blue} method for the case of constant density in spectral space.
We also provide evaluations of the spatial properties in Table~\ref{comparing-table}.
Furthermore,
we also make an evaluation of blue noise sampling for an intensity ramp in Fig.~\ref{one-class-sampling}
by counting the number of points for each
quarter of the ramp.
It can be shown that our algorithm is comparable to CCVT~\cite{balzer:2009:capacity} and BOT~\cite{de:2012:blue} for both the cases of constant density and the intensity ramp.

\textbf{Multi-class sampling on constant density.}
To evaluate blue noise distribution for multi-class sampling,
we first analyse the simplest case where all classes have the constant density function and same number of samples.
Fig.~\ref{two-class-sampling}, Fig.~\ref{three-class-sampling},
and Fig.~\ref{seven-class-sampling} illustrate the result generated by our method and Wei's ~\cite{wei:2010:multi} on two-class, three-class, and seven-class sampling separately.
In two-class sampling (Fig.~\ref{two-class-sampling}),
we illustrate the influence of the weighted parameter $\lambda_i$ on the trade-off between good distribution of the samples belonging to each individual class and good distribution of total samples.
Large $\lambda_i$ preserves the blue noise property of the $i$-$th$ well.
Depending on which distribution is more important,
$\lambda_i$ can be chosen for different applications.
When $\lambda_1+\lambda_2=\lambda_3$,
the balance between good distribution of the samples belonging to each individual class and good distribution of total samples
is achieved in Fig.~\ref{two-class-sampling-c}.
Fig.~\ref{two-class-sampling-a} and Fig.~\ref{two-class-sampling-c},
show that our results are comparable to that of Wei's.
In three-class sampling (Fig.~\ref{three-class-sampling}),
we evaluate blue noise distribution of samples of different combined classes.
Compared with Wei's method~\cite{wei:2010:multi},
 blue noise profile of the combined class with two individual class is improved in our method
while blue noise distribution of each individual class is preserved.
In seven-class sampling (Fig.~\ref{seven-class-sampling}),
blue noise profile  is still kept for samples of each individual class and total class.
It shows that our method is feasible for more numbers of class sampling.

% 做一个
To evaluate efficiency of relaxing conflicts of desired centroid locations in our approach,
we compare our approach with Wei's method~\cite{wei:2010:multi} on the
difference between centroid locations of voronoi diagrams for single class samples and multi-class samples in Fig.~\ref{conflict-evaluation}.
We apply conflicts ratio to evaluate the difference between desired centroid locations.
Conflicts ratio is defined as 
\begin{equation*}
  R=\sum\limits_{i=1}^n\frac{||C_i^I-C_i^C||}{nD}
\end{equation*}
where $C_i^I$ and $C_i^C$ are centroid locations of voronoi diagrams of the $i-th$ sample for individual class samples and combined class samples,
$D=\sum\limits_{i=1}^nD_i/n$ the mean of nearest neighbor distances for every samples and $D_i$ is the distance between the $i-th$ sample and its nearest neighbor sample,
$n$ is the number of samples.


\textbf{Adaptive multi-class sampling on ramp.}
To evaluate multi-class blue noise sampling on non-uniform density function,
we applied two intensity ramp and count the number of points for each quarter of the ramp.
Fig.~\ref{adaptive sampling} shows samples generated by
our method and Wei's~\cite{wei:2010:multi}.
While samples of every individual class and the combined class represent approximately the right counting of points per quarter,
it can be seen that our results show noticeably less noise.

\textbf{Multi-class sampling on point set surface.}
Our method can be directly applied to multi-class sampling on point set surface.
We assume that original points represent a discrete probability measure $\nu=\sum\limits_{m=1}^{M}\rho_m\delta_{x_m}$ as Eq.~\ref{density-function} on the surface.
$\rho_m$ is set as the normalized area element at the point $x_m$ for multi-class blue noise sampling with constant density function.
$D_{m,j}$ is the geodesic distance.
For convenience,
we applied the Euclidean distance instead of the geodesic distance in this paper.
Samples are initialized on the surface.
After each iteration,
every sample is mapped back onto the surface by
moving least square projection~\cite{alexa:2001:point}.
In Fig.~\ref{bunny-sampling},
we make a two-class blue noise sampling on a bunny model,
and show the Differential Domain Function~\cite{wei:2011:differential} to demonstrate blue noise profile of different class sample distributions.

\textbf{Multi-class blue noise sampling with different sizes.}
Analogous to setting different radius of Possion disks for different class in dart throwing,
different sizes of samples can be set  via discrete probability measure $\bar\mu_k$ for different classes in our method.
Suppose that the same constant density function $\rho_i (1\leq i\leq n)$ is used for different classes,
we set the discrete probability density as:
\begin{equation}\label{adaptive-size}
  \bar\rho_{i,j}^k= \left\{
  \begin{array} {cl}
  \frac{1}{n_kN_i} & \exists k_l = i \\
  0 & others
  \end{array} \right..
\end{equation}
When two-class sampling is done with $N_1$ and $N_2$ samples ($N_1>N_2$) ,
every sample belonging to the $2$-th class is given more measure than every sample belonging to the $1$-th in the combined class according to Eq.~\ref{adaptive-size}.
When the density function of the combined class is a constant function,
the $2$-th sample is with larger size than the $1$-th sample.
The radius of every sample can be approximated as:
\begin{equation*}
  r_i=\lambda r_{i_{max}}/n
\end{equation*}
where $r_{i_max}$ is the maximun possible disk radius of the $i$-th class,
$\lambda$ is a relative radius parameter.
In our experiments,
we found that there is on overlap between any pair samples when $0\leq\lambda\leq 0.5$.
In Fig.~\ref{adaptive sampling},
our method is applied for three-class blue noise sampling with different size on a square and a two dimensional manifold.



\textbf{Complexity.}
One iteration of our algorithm involves iterative bregman projection.
The time complexity depends on what classes are involved in the framework.
For a single class sampling,
the time complexity is $O(MN)$.
Although this is worse than $O(NlogN+M)$,
iterative bregman projection can be easily parallelized on the GPU.
When only a total set is used as a combined class,
the time complexity is $O(2MN)$.
If all $2^n-1$ classes are involved,
the complexity is $O(2^{n-1}MN)$.
For a general application,
only a combined class of the total set is enough to generate a good point distribution.
Thus, it is significantly better than CapCVT~\cite{chen:2012:variational} which is $O(nMN+nN^2)$.
While our algorithm performs well for small datasets,
our method is limited by its memory requirements $O(2MN)$ for large datasets, such as the experiment in Fig.~\ref{Color stippling}.

\textbf{Performance.}
% 用的什么设备
% 针对单类做了什么，结论是什么
% 针对多类做了什么，结论是什么
All of our performance are locked on a workstation with Intel Xeon 3.50GHz
dual-core CPUs and 32GB memory, and NVIDIA Quadro K5000 GPU with 2GB memory.
For single blue noise sampling,
we compare our method with traditional CCVT~\cite{balzer:2009:capacity}
and power diagrams method~\cite{de:2012:blue} on running time in Table~\ref{time-table-singleclass}
Since the most time is spend for integral computation in power diagrams method~\cite{de:2012:blue} and distance matrix computation in our approach,
CCVT~\cite{balzer:2009:capacity} performs more better in running time than these two method when sampling rate is lower than $1/64$.
With the increasing of sampling points or the decreasing of sampling rate,
our approach outperforms CCVT~\cite{balzer:2009:capacity} and power diagrams~\cite{de:2012:blue}.
With the increasing of sampling points,
our approach is more and more faster than CCVT~\cite{balzer:2009:capacity},
and is two times faster than CCVT~\cite{balzer:2009:capacity} at least.
With the increasing of sampling points,
our approach is faster than power diagrams method~\cite{de:2012:blue} by over $10\%$.
For mutli-class blue noise sampling,
we compare our method with Wei's method~\cite{wei:2010:multi} on running time in Table~\ref{time-table-multiclass}.
Our approach is two times faster than Wei's method~\cite{wei:2010:multi} at least.
In our algorithm,
we use a global distance matrix.
In every iteration,
Updating the matrix spends most of running time.
However, the matrix is a sparse matrix.
Thus, we can furthermore improve the performance of our approach by making use of a sparse distance matrix.


%We measure the timing via a uniform sampling.
%The number of all samples is 2048.
%The number of discrete points $M=128\times 128$.
%Only total set is considered as a combined class.
%Table. shows the computation times of our algorithm.

